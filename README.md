## Welcome to GitHub Pages

𝐓𝐡𝐢𝐬 𝐑𝐞𝐩𝐨𝐬𝐢𝐭𝐨𝐫𝐲 𝐜𝐨𝐧𝐭𝐚𝐢𝐧 𝐭𝐡𝐞 𝐌𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐚𝐥 𝐅𝐨𝐫𝐦𝐮𝐥𝐚𝐭𝐢𝐨𝐧 𝐁𝐞𝐡𝐢𝐧𝐝 𝐍 𝐭𝐨 𝐍 𝐀𝐫𝐭𝐢𝐟𝐢𝐜𝐢𝐚𝐥 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤 .
*       𝟏) 𝐌𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐬 𝐁𝐞𝐡𝐢𝐧𝐝 𝐅𝐨𝐫𝐰𝐚𝐫𝐝 𝐏𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧
*       𝟐) 𝐌𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐬 𝐁𝐞𝐡𝐢𝐧𝐝 𝐋𝐨𝐬𝐬
*       𝟑) 𝐌𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐬 𝐁𝐞𝐡𝐢𝐧𝐝 𝐁𝐚𝐜𝐤𝐰𝐚𝐫𝐝 𝐏𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 


### 𝐅𝐨𝐫𝐰𝐚𝐫𝐝 𝐏𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧


ʟᴇᴛ ᴜꜱ ɢᴇᴛ ᴛᴏ ᴛʜᴇ ᴛᴏᴘɪᴄ ᴅɪʀᴇᴄᴛʟʏ. ᴇxᴀᴄᴛʟʏ ᴡʜᴀᴛ ɪꜱ ꜰᴏʀᴡᴀʀᴅ ᴘʀᴏᴘᴀɢᴀᴛɪᴏɴ ɪɴ ɴᴇᴜʀᴀʟ ɴᴇᴛᴡᴏʀᴋꜱ? ᴡᴇʟʟ, ɪꜰ ʏᴏᴜ ʙʀᴇᴀᴋ ᴅᴏᴡɴ ᴛʜᴇ ᴡᴏʀᴅꜱ, ꜰᴏʀᴡᴀʀᴅ ɪᴍᴘʟɪᴇꜱ ᴍᴏᴠɪɴɢ ᴀʜᴇᴀᴅ ᴀɴᴅ ᴘʀᴏᴘᴀɢᴀᴛɪᴏɴ ɪꜱ ᴀ  ᴛᴇʀᴍ ꜰᴏʀ ꜱᴀʏɪɴɢ ꜱᴘʀᴇᴀᴅɪɴɢ ᴏꜰ ᴀɴʏᴛʜɪɴɢ. ꜰᴏʀᴡᴀʀᴅ ᴘʀᴏᴘᴀɢᴀᴛɪᴏɴ ᴍᴇᴀɴꜱ ᴡᴇ ᴀʀᴇ ᴍᴏᴠɪɴɢ ɪɴ ᴏɴʟʏ ᴏɴᴇ ᴅɪʀᴇᴄᴛɪᴏɴ, ꜰʀᴏᴍ ɪɴᴘᴜᴛ ᴛᴏ ᴛʜᴇ ᴏᴜᴛᴘᴜᴛ, ɪɴ ᴀ ɴᴇᴜʀᴀʟ ɴᴇᴛᴡᴏʀᴋ. 

𝐋𝐞𝐭𝐬 𝐮𝐬 𝐭𝐚𝐥𝐤 𝐡𝐨𝐰 𝐭𝐨 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐞 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐢𝐧 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤 .
𝐓𝐨 𝐝𝐨 𝐭𝐡𝐚𝐭 𝐟𝐢𝐫𝐬𝐭 𝐰𝐞 𝐮𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝 𝐚 𝐩𝐚𝐫𝐭 𝐨𝐟 𝐅𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 𝐢𝐧 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤 𝐰𝐡𝐢𝐜𝐡 𝐢𝐬 𝐜𝐚𝐥𝐥𝐞𝐝 𝐏𝐞𝐫𝐜𝐞𝐩𝐭𝐫𝐨𝐧.


![1_inTTvvOl2DfRWgj4Zc2sBg](https://user-images.githubusercontent.com/76767487/147902680-e378e9bc-3a13-4b01-9425-57dd1cdc8ac6.png)

# 𝐏𝐚𝐫𝐭𝐬 𝐨𝐟 𝐭𝐡𝐞 𝐬𝐢𝐦𝐩𝐥𝐞 𝐩𝐞𝐫𝐜𝐞𝐩𝐭𝐫𝐨𝐧 𝐢𝐬 𝐈𝐧𝐩𝐮𝐭 , 𝐁𝐢𝐚𝐬  , 𝐖𝐞𝐢𝐠𝐡𝐭𝐬 , 𝐒𝐮𝐦𝐦𝐚𝐭𝐢𝐨𝐧 𝐅𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐀𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐎𝐮𝐭𝐩𝐮𝐭 , 𝐋𝐨𝐬𝐬.

# 𝟏) 𝐈𝐧𝐩𝐮𝐭 : 
𝐓𝐡𝐞 𝐢𝐧𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 𝐢𝐬 𝐭𝐡𝐞 𝐯𝐞𝐫𝐲 𝐛𝐞𝐠𝐢𝐧𝐧𝐢𝐧𝐠 𝐨𝐟 𝐭𝐡𝐞 𝐰𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐟𝐨𝐫 𝐭𝐡𝐞 𝐚𝐫𝐭𝐢𝐟𝐢𝐜𝐢𝐚𝐥 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤. 𝐃𝐚𝐭𝐚 𝐢𝐬 𝐭𝐚𝐤𝐞𝐧 𝐚𝐬 𝐢𝐧𝐩𝐮𝐭 𝐢𝐧 𝐢𝐧𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 . 𝐟𝐨𝐫 𝐞𝐱𝐚𝐦𝐩𝐥𝐞 𝐌𝐍𝐈𝐒𝐓 𝐃𝐚𝐭𝐚𝐬𝐞𝐭 𝐰𝐡𝐢𝐜𝐡 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐬 𝟔𝟎𝟎𝟎𝟎 𝐡𝐚𝐧𝐝𝐰𝐫𝐢𝐭𝐭𝐞𝐧 𝐧𝐮𝐦𝐛𝐞𝐫𝐬 𝐢𝐦𝐚𝐠𝐞𝐬 𝐚𝐧𝐝 𝐞𝐚𝐜𝐡 𝐢𝐦𝐚𝐠𝐞𝐬 𝐡𝐚𝐯𝐞 𝟐𝟖 𝐱 𝟐𝟖 𝐩𝐢𝐱𝐞𝐥 . 𝐬𝐨 𝐰𝐞 𝐰𝐢𝐥𝐥 𝐜𝐨𝐧𝐯𝐞𝐫𝐭 𝐭𝐡𝐞 𝟐 𝐝𝐢𝐦𝐞𝐧𝐬𝐢𝐨𝐧𝐚𝐥 𝐚𝐫𝐫𝐚𝐭 𝐢𝐧𝐭𝐨 𝐬𝐢𝐧𝐠𝐥𝐞 𝐚𝐫𝐫𝐚𝐲 𝐞.𝐠 𝟐𝟖𝐱𝟐𝟖= 𝟕𝟖𝟒 𝐚𝐧𝐝 𝐟𝐞𝐞𝐝 𝐢𝐧 𝐍𝐞𝐮𝐫𝐚𝐥 𝐍𝐞𝐭𝐰𝐨𝐫𝐤  𝐢𝐧 𝐨𝐮𝐭 𝐢𝐧𝐩𝐮𝐭 𝐥𝐚𝐲𝐞𝐫 𝐦𝐞𝐚𝐧𝐬 𝐭𝐡𝐞𝐫𝐞 𝐰𝐢𝐥𝐥 𝐛𝐞 𝐮𝐩𝐭𝐨 𝐢𝐧𝐩𝐮𝐭𝟕𝟖𝟒 𝐢𝐧 𝐚𝐛𝐨𝐯𝐞 𝐢𝐦𝐚𝐠𝐞
𝐞𝐚𝐜𝐡 𝐢𝐧𝐩𝐮𝐭 𝐝𝐚𝐭𝐚 𝐢𝐬 𝐜𝐚𝐥𝐥𝐞𝐝 𝐮𝐧𝐢𝐭

# 𝟐) 𝐁𝐢𝐚𝐬 : 
𝐁𝐢𝐚𝐬 𝐢𝐬 𝐧𝐨𝐭𝐡𝐢𝐧𝐠 𝐛𝐮𝐭 𝐚 𝐜𝐨𝐧𝐬𝐭𝐚𝐧𝐭 𝐰𝐡𝐢𝐜𝐡 𝐡𝐞𝐥𝐩𝐬 𝐮𝐬 𝐭𝐨 𝐒𝐡𝐢𝐟𝐭 𝐨𝐮𝐫 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐮𝐩𝐰𝐚𝐫𝐝 𝐨𝐫 𝐃𝐨𝐰𝐧𝐰𝐚𝐫𝐝 𝐟𝐨𝐫 𝐞𝐱𝐚𝐦𝐩𝐥𝐞 
𝐈𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐨𝐮𝐫 𝐁𝐢𝐚𝐬 𝐢𝐬 𝐳𝐞𝐫𝐨 , 𝐚𝐧𝐝 𝐛𝐞𝐥𝐨𝐰 𝐢𝐦𝐚𝐠𝐞 𝐬𝐡𝐨𝐰 𝐭𝐡𝐚𝐭 𝐨𝐮𝐫 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐝𝐨𝐞𝐬 𝐧𝐨𝐭 𝐟𝐢𝐭 𝐰𝐞𝐥𝐥 .
𝐈𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐛𝐢𝐚𝐬 𝐢𝐬 𝐬𝐞𝐭 𝐭𝐨 𝐫𝐚𝐧𝐝𝐨𝐦 𝐯𝐚𝐥𝐮𝐞 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝟎 𝐭𝐨 𝟏 𝐨𝐫 𝟎 𝐨𝐫 -𝟏 𝐭𝐨 𝟏 . 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐠𝐞𝐭𝐬 𝐜𝐡𝐚𝐧𝐠𝐞𝐝 𝐢𝐧 𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 𝐭𝐨 𝐠𝐞𝐭 𝐭𝐡𝐞 𝐛𝐞𝐬𝐭 𝐛𝐢𝐚𝐬 𝐯𝐚𝐥𝐮𝐞 .

<img src="https://user-images.githubusercontent.com/76767487/147926230-65b5d87f-3d6f-49f3-b724-b8256099b710.jpeg" width="600" height="400">

𝐓𝐨 𝐅𝐢𝐭 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐰𝐞𝐥𝐥  𝐰𝐞 𝐧𝐞𝐞𝐝 𝐭𝐨 𝐬𝐡𝐢𝐟𝐭 𝐨𝐮𝐫 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐢𝐧 𝐮𝐩𝐰𝐚𝐫𝐝 𝐝𝐢𝐫𝐞𝐜𝐭𝐢𝐨𝐧 . 𝐈𝐭 𝐜𝐚𝐧 𝐛𝐞 𝐝𝐨𝐧𝐞 𝐛𝐲 𝐢𝐧𝐜𝐫𝐞𝐦𝐞𝐧𝐭𝐢𝐧𝐠 𝐛𝐢𝐚𝐬 . (𝐞.𝐠 𝐛𝐢𝐚𝐬=𝟏 )

<img src="https://user-images.githubusercontent.com/76767487/147926234-d7e487ab-79a3-4de6-b405-efb8dacdd567.jpeg" width="600" height="400">

# 𝟑) 𝐖𝐞𝐢𝐠𝐡𝐭𝐬 :
𝐖𝐞𝐢𝐠𝐡𝐭 𝐢𝐬 𝐭𝐡𝐞 𝐩𝐚𝐫𝐚𝐦𝐞𝐭𝐞𝐫 𝐰𝐢𝐭𝐡𝐢𝐧 𝐚 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐭𝐡𝐚𝐭 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐬 𝐢𝐧𝐩𝐮𝐭 𝐝𝐚𝐭𝐚 𝐰𝐢𝐭𝐡𝐢𝐧 𝐭𝐡𝐞 𝐧𝐞𝐭𝐰𝐨𝐫𝐤'𝐬 𝐡𝐢𝐝𝐝𝐞𝐧 𝐥𝐚𝐲𝐞𝐫𝐬. 𝐀 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐢𝐬 𝐚 𝐬𝐞𝐫𝐢𝐞𝐬 𝐨𝐟 𝐧𝐨𝐝𝐞𝐬, 𝐨𝐫 𝐧𝐞𝐮𝐫𝐨𝐧𝐬. 𝐖𝐢𝐭𝐡𝐢𝐧 𝐞𝐚𝐜𝐡 𝐧𝐨𝐝𝐞 𝐢𝐬 𝐚 𝐬𝐞𝐭 𝐨𝐟 𝐢𝐧𝐩𝐮𝐭𝐬, 𝐰𝐞𝐢𝐠𝐡𝐭, 𝐚𝐧𝐝 𝐚 𝐛𝐢𝐚𝐬 𝐯𝐚𝐥𝐮𝐞. ... 𝐎𝐟𝐭𝐞𝐧 𝐭𝐡𝐞 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐨𝐟 𝐚 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐚𝐫𝐞 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐞𝐝 𝐰𝐢𝐭𝐡𝐢𝐧 𝐭𝐡𝐞 𝐡𝐢𝐝𝐝𝐞𝐧 𝐥𝐚𝐲𝐞𝐫𝐬 𝐨𝐟 𝐭𝐡𝐞 𝐧𝐞𝐭𝐰𝐨𝐫𝐤.
𝐈𝐧𝐢𝐭𝐢𝐚𝐥𝐥𝐲 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐨𝐟 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐢𝐬 𝐬𝐞𝐭 𝐭𝐨 𝐫𝐚𝐧𝐝𝐨𝐦 𝐯𝐚𝐥𝐮𝐞 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝟎 𝐭𝐨 𝟏  𝐨𝐫 -𝟏 𝐭𝐨 𝟏 . 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐠𝐞𝐭𝐬 𝐜𝐡𝐚𝐧𝐠𝐞𝐝 𝐢𝐧 𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐩𝐚𝐠𝐚𝐭𝐢𝐨𝐧 𝐭𝐨 𝐠𝐞𝐭 𝐭𝐡𝐞 𝐛𝐞𝐬𝐭 𝐰𝐞𝐢𝐠𝐡𝐭 𝐯𝐚𝐥𝐮𝐞 .

# 4) Summation Function :
𝐀 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐭𝐡𝐚𝐭 𝐜𝐨𝐦𝐛𝐢𝐧𝐞𝐬 𝐭𝐡𝐞 𝐯𝐚𝐫𝐢𝐨𝐮𝐬 𝐢𝐧𝐩𝐮𝐭 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧𝐬 𝐢𝐧𝐭𝐨 𝐚 𝐬𝐢𝐧𝐠𝐥𝐞 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 . 𝐓𝐡𝐞 𝐩𝐨𝐢𝐧𝐭 𝐨𝐟 𝐜𝐨𝐧𝐭𝐚𝐜𝐭 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝐚𝐝𝐣𝐚𝐜𝐞𝐧𝐭 𝐧𝐞𝐮𝐫𝐨𝐧𝐬 𝐰𝐡𝐞𝐫𝐞 𝐧𝐞𝐫𝐯𝐞 𝐢𝐦𝐩𝐮𝐥𝐬𝐞𝐬 𝐚𝐫𝐞 𝐭𝐫𝐚𝐧𝐬𝐦𝐢𝐭𝐭𝐞𝐝 𝐟𝐫𝐨𝐦 𝐨𝐧𝐞 𝐭𝐨 𝐚𝐧𝐨𝐭𝐡𝐞𝐫.
 𝐅𝐨𝐫𝐦𝐮𝐥𝐚 𝐟𝐨𝐫 𝐬𝐮𝐦𝐦𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐢𝐬 𝐬𝐮𝐦 𝐨𝐟 𝐭𝐡𝐞 𝐩𝐫𝐨𝐝𝐮𝐜𝐭 𝐨𝐟 𝐭𝐡𝐞 𝐢𝐧𝐩𝐮𝐭 𝐚𝐧𝐝 𝐭𝐡𝐞𝐢𝐫 𝐚𝐬𝐬𝐨𝐜𝐢𝐚𝐭𝐞𝐝 𝐰𝐞𝐢𝐠𝐡𝐭𝐬 𝐩𝐥𝐮𝐬 𝐛𝐢𝐚𝐬 .
 
 <img src="https://user-images.githubusercontent.com/76767487/147933660-f3305322-33b4-4cbb-8125-fea0f8a46730.png" width="700" height="400">
 
𝐬𝐮𝐦 = 𝐰𝟏 * 𝐱𝟏 +  𝐰𝟐 * 𝐱𝟐 + ... + 𝐰𝐧 * 𝐱𝐧   + 𝐛𝐢𝐚𝐬     (:.  𝐡𝐞𝐫𝐞 𝐛𝐢𝐚𝐬 𝐢𝐬 𝐧𝐨𝐭 𝐠𝐢𝐯𝐞𝐧 𝐰𝐞 𝐜𝐚𝐧 𝐚𝐝𝐝 𝐛𝐢𝐚𝐬 𝐥𝐢𝐤𝐞 𝐟𝐢𝐫𝐬𝐭 𝐢𝐦𝐚𝐠𝐞 )
:. 𝐡𝐞𝐫𝐞 𝐭𝐡𝐫𝐞𝐬𝐡𝐨𝐥𝐝 𝐓 𝐢𝐬 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧

# 5) Activation Function :
𝐀𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐅𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐡𝐞𝐥𝐩𝐬 𝐭𝐡𝐞 𝐧𝐞𝐮𝐫𝐚𝐥 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐭𝐨 𝐮𝐬𝐞 𝐢𝐦𝐩𝐨𝐫𝐭𝐚𝐧𝐭 𝐢𝐧𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐰𝐡𝐢𝐥𝐞 𝐬𝐮𝐩𝐩𝐫𝐞𝐬𝐬𝐢𝐧𝐠 𝐢𝐫𝐫𝐞𝐥𝐞𝐯𝐚𝐧𝐭 𝐝𝐚𝐭𝐚 𝐩𝐨𝐢𝐧𝐭𝐬. 
𝐀𝐧 𝐀𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐅𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐝𝐞𝐜𝐢𝐝𝐞𝐬 𝐰𝐡𝐞𝐭𝐡𝐞𝐫 𝐚 𝐧𝐞𝐮𝐫𝐨𝐧 𝐬𝐡𝐨𝐮𝐥𝐝 𝐛𝐞 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐞𝐝 𝐨𝐫 𝐧𝐨𝐭. 𝐓𝐡𝐢𝐬 𝐦𝐞𝐚𝐧𝐬 𝐭𝐡𝐚𝐭 𝐢𝐭 𝐰𝐢𝐥𝐥 𝐝𝐞𝐜𝐢𝐝𝐞 𝐰𝐡𝐞𝐭𝐡𝐞𝐫 𝐭𝐡𝐞 𝐧𝐞𝐮𝐫𝐨𝐧’𝐬 𝐢𝐧𝐩𝐮𝐭 𝐭𝐨 𝐭𝐡𝐞 𝐧𝐞𝐭𝐰𝐨𝐫𝐤 𝐢𝐬 𝐢𝐦𝐩𝐨𝐫𝐭𝐚𝐧𝐭 𝐨𝐫 𝐧𝐨𝐭 𝐢𝐧 𝐭𝐡𝐞 𝐩𝐫𝐨𝐜𝐞𝐬𝐬 𝐨𝐟 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧 𝐮𝐬𝐢𝐧𝐠 𝐬𝐢𝐦𝐩𝐥𝐞𝐫 𝐦𝐚𝐭𝐡𝐞𝐦𝐚𝐭𝐢𝐜𝐚𝐥 𝐨𝐩𝐞𝐫𝐚𝐭𝐢𝐨𝐧𝐬. 
𝐓𝐡𝐞𝐫𝐞 𝐚𝐫𝐞 𝐥𝐨𝐭𝐬 𝐨𝐟 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 , 𝐟𝐚𝐦𝐨𝐮𝐬 𝐚𝐫𝐞 𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐚𝐧𝐝 𝐫𝐞𝐥𝐮 .
𝐬𝐢𝐠𝐦𝐨𝐢𝐝 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐭𝐚𝐤𝐞 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐚𝐬 𝐢𝐧𝐩𝐮𝐭 (𝐞𝐠 𝐚𝐧𝐲 +𝐢𝐯𝐞 𝐫𝐞𝐚𝐥 𝐧𝐮𝐦𝐛𝐞𝐫 ) 𝐚𝐧𝐝 𝐜𝐨𝐧𝐯𝐞𝐫𝐭 𝐢𝐧𝐭𝐨 𝐧𝐮𝐦𝐛𝐞𝐫 𝐛𝐞𝐭𝐰𝐞𝐞𝐧 𝟎 𝐭𝐨 𝟏.
𝐫𝐞𝐥𝐮 𝐚𝐜𝐭𝐢𝐯𝐚𝐭𝐢𝐨𝐧 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧 𝐭𝐚𝐤𝐞 𝐭𝐡𝐞 𝐯𝐚𝐥𝐮𝐞 𝐚𝐬 𝐢𝐧𝐩𝐮𝐭 (𝐞𝐠 𝐚𝐧𝐲 𝐫𝐞𝐚𝐥 𝐧𝐮𝐦𝐛𝐞𝐫 ) 𝐢𝐟 𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐬 𝐠𝐫𝐞𝐚𝐭𝐞𝐫 𝐭𝐡𝐚𝐧 𝟎 𝐭𝐡𝐞𝐧 𝐬𝐢𝐦𝐩𝐥𝐞 𝐫𝐞𝐭𝐮𝐫𝐧 𝐭𝐡𝐞 𝐨𝐫𝐢𝐠𝐢𝐧𝐚𝐥 𝐯𝐚𝐥𝐮𝐞 𝐢𝐟 𝐧𝐮𝐦𝐛𝐞𝐫 𝐢𝐬 𝐥𝐞𝐬𝐬 𝐭𝐡𝐚𝐧 𝟎 𝐭𝐡𝐞𝐧 𝐢𝐭 𝐫𝐞𝐭𝐮𝐫𝐧 𝟎.

<img src="https://user-images.githubusercontent.com/76767487/147934683-a9fb68fe-4fbe-425f-8a3c-b7cd7d826b52.png" width="700" height="400">

𝐟𝐨𝐫 𝐬𝐢𝐠𝐦𝐨𝐢𝐝 , 
𝐨𝐮𝐭𝐩𝐮𝐭 = 𝟏 / (𝟏 + 𝐞**(-𝐬𝐮𝐦) ) , 𝐚𝐧𝐝 𝐭𝐡𝐢𝐬 𝐨𝐮𝐭𝐩𝐮𝐭 𝐢𝐬 𝐭𝐚𝐤𝐞𝐧 𝐚𝐬 𝐢𝐧𝐩𝐮𝐭 𝐟𝐨𝐫 𝐧𝐞𝐱𝐭 𝐥𝐚𝐲𝐞𝐫 𝐚𝐧𝐝 𝐬𝐨 𝐨𝐧 , 𝐎𝐧 𝐥𝐚𝐫𝐠𝐞 𝐬𝐜𝐚𝐥𝐞

<img src="https://user-images.githubusercontent.com/76767487/147939001-48c50547-6d7e-4ea8-bbab-9d90a804903a.jpeg" width="700" height="1200">

# 6 ) Output :
𝐥𝐚𝐬𝐭 𝐚𝐧𝐬𝐰𝐞𝐫 𝐟𝐫𝐨𝐦 𝐥𝐚𝐬𝐭 𝐮𝐧𝐢𝐭 𝐢𝐧 𝐥𝐚𝐬𝐭 𝐥𝐚𝐲𝐞𝐫 . 

# 7) Loss : 
In above image the answer is given e.g 0.99 and 
 













```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [Basic writing and formatting syntax](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/VandanVirani/Mathematics-For-Artificial_Neural_Network.github.io/settings/pages). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://support.github.com/contact) and we’ll help you sort it out.
